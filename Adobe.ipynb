{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NcYCmkLLORy",
        "outputId": "ca7fcff7-e7eb-4f20-e859-3259efb3b205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.11/dist-packages (20250506)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf2\n",
            "Successfully installed pypdf2-3.0.1\n",
            "Collecting anytree\n",
            "  Downloading anytree-2.13.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Downloading anytree-2.13.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anytree\n",
            "Successfully installed anytree-2.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf2 pdfminer.six pdfplumber\n",
        "!pip install anytree  # for hierarchical structure visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "def extract_outline(pdf_path):\n",
        "    \"\"\"\n",
        "    Enhanced version to extract title and headings (H1-H4) from a PDF document\n",
        "    with improved heading detection and level assignment\n",
        "    \"\"\"\n",
        "    outline = []\n",
        "    title = \"\"\n",
        "    previous_level = None\n",
        "    heading_counts = defaultdict(int)\n",
        "\n",
        "    # Heading patterns with improved matching\n",
        "    h1_pattern = re.compile(r'^(?![A-Za-z]\\.\\s)[A-Z][A-Z0-9\\s\\-—]+$')  # All caps with possible numbers/dashes\n",
        "    h2_pattern = re.compile(r'^(?![A-Za-z]\\.\\s)[A-Z][a-zA-Z0-9\\s,:;—\\-]+$')  # Title case\n",
        "    h3_pattern = re.compile(r'^(?![A-Za-z]\\.\\s)[a-zA-Z0-9\\s]+:$')  # Ends with colon\n",
        "    h4_pattern = re.compile(r'^(?![A-Za-z]\\.\\s)[a-zA-Z0-9\\s]+(\\?|—|:)$')  # Ends with ? or — or :\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        # First try to get title from first line of first page\n",
        "        first_page = pdf.pages[0]\n",
        "        first_text = first_page.extract_text()\n",
        "        if first_text:\n",
        "            # Look for RFP pattern in first few lines\n",
        "            for line in first_text.split('\\n')[:5]:\n",
        "                if \"RFP:\" in line or \"Request for Proposal\" in line:\n",
        "                    title = line.strip()\n",
        "                    break\n",
        "            if not title:\n",
        "                title = first_text.split('\\n')[0].strip()\n",
        "\n",
        "        # Process all pages for headings\n",
        "        for page_num, page in enumerate(pdf.pages, start=1):\n",
        "            text = page.extract_text()\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            lines = text.split('\\n')\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "\n",
        "                # Skip common non-heading items\n",
        "                if (len(line.split()) > 10 or  # Too long for a heading\n",
        "                    line.isdigit() or  # Page numbers\n",
        "                    re.match(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}$', line) or  # Dates\n",
        "                    re.match(r'^[A-Za-z]\\.\\s', line) or  # List items\n",
        "                    line.startswith('http') or  # URLs\n",
        "                    len(line) > 150 or  # Too long\n",
        "                    re.search(r'([A-Z])\\1{3,}', line)):  # Repeated characters\n",
        "                    continue\n",
        "\n",
        "                # Determine heading level with priority\n",
        "                level = None\n",
        "                if h1_pattern.match(line) and len(line.split()) <= 8:\n",
        "                    level = \"H1\"\n",
        "                elif h2_pattern.match(line) and len(line.split()) <= 8:\n",
        "                    level = \"H2\"\n",
        "                elif h3_pattern.match(line) and len(line.split()) <= 6:\n",
        "                    level = \"H3\"\n",
        "                elif h4_pattern.match(line) and len(line.split()) <= 6:\n",
        "                    level = \"H4\"\n",
        "\n",
        "                # Additional context checks\n",
        "                if level:\n",
        "                    # Count occurrences to help with level adjustment\n",
        "                    heading_counts[level] += 1\n",
        "\n",
        "                    # Adjust level based on document structure\n",
        "                    if level == \"H1\" and heading_counts[\"H1\"] > 3:\n",
        "                        level = \"H2\"\n",
        "                    elif level == \"H2\" and previous_level == \"H1\":\n",
        "                        pass  # Keep as H2\n",
        "                    elif level == \"H2\" and previous_level == \"H3\":\n",
        "                        level = \"H3\"\n",
        "\n",
        "                    # Skip if same level appears consecutively with similar length\n",
        "                    if (previous_level and\n",
        "                        level == previous_level and\n",
        "                        len(outline) > 0 and\n",
        "                        outline[-1]['page'] == page_num and\n",
        "                        abs(len(outline[-1]['text']) - len(line)) < 10):\n",
        "                        continue\n",
        "\n",
        "                    outline.append({\"level\": level, \"text\": line, \"page\": page_num})\n",
        "                    previous_level = level\n",
        "\n",
        "    # Post-processing to clean up results\n",
        "    cleaned_outline = []\n",
        "    skip_phrases = [\"March 21, 2003\", \"April 21, 2003\", \"Timeline:\", \"Committee.\"]\n",
        "\n",
        "    for item in outline:\n",
        "        # Skip specific phrases and page numbers\n",
        "        if (item['text'] in skip_phrases or\n",
        "            re.match(r'^Page \\d+$', item['text']) or\n",
        "            re.match(r'^\\d+$', item['text'])):\n",
        "            continue\n",
        "\n",
        "        # Clean up text\n",
        "        text = item['text'].strip()\n",
        "        if not text.endswith((':', '?', '—')):\n",
        "            text = text.rstrip('.')\n",
        "\n",
        "        cleaned_outline.append({\n",
        "            \"level\": item['level'],\n",
        "            \"text\": text,\n",
        "            \"page\": item['page']\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"title\": title.strip(),\n",
        "        \"outline\": cleaned_outline\n",
        "    }\n",
        "\n",
        "def process_pdf(input_path, output_path):\n",
        "    \"\"\"Process a PDF file and save the outline as JSON\"\"\"\n",
        "    result = extract_outline(input_path)\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(result, f, indent=2)\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "pdf_path = \"/content/file05.pdf\"  # Replace with your PDF path\n",
        "output_json = \"output.json\"\n",
        "result = process_pdf(pdf_path, output_json)\n",
        "print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yux94q2fLxxb",
        "outputId": "52b6913c-8de7-4f47-e543-676e0f9a7b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"title\": \"ADDRESS:\",\n",
            "  \"outline\": [\n",
            "    {\n",
            "      \"level\": \"H2\",\n",
            "      \"text\": \"ADDRESS:\",\n",
            "      \"page\": 1\n",
            "    },\n",
            "    {\n",
            "      \"level\": \"H1\",\n",
            "      \"text\": \"TOPJUMP\",\n",
            "      \"page\": 1\n",
            "    },\n",
            "    {\n",
            "      \"level\": \"H2\",\n",
            "      \"text\": \"PIGEON FORGE, TN 37863\",\n",
            "      \"page\": 1\n",
            "    },\n",
            "    {\n",
            "      \"level\": \"H1\",\n",
            "      \"text\": \"CLOSED TOED SHOES ARE REQUIRED FOR CLIMBING\",\n",
            "      \"page\": 1\n",
            "    },\n",
            "    {\n",
            "      \"level\": \"H2\",\n",
            "      \"text\": \"PARENTS OR GUARDIANS NOT ATTENDING THE PARTY,\",\n",
            "      \"page\": 1\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber transformers sentence-transformers nltk\n",
        "!python -m nltk.downloader punkt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "97asfbGplxaG",
        "outputId": "bf156714-a119-4a64-e6a9-cd031a7e23c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12 nvidia-cusolver-cu12-11.6.1.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "d0e961756f594124bf6c6cc00dee76cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "class PDFRanker:\n",
        "    def __init__(self):\n",
        "        # Load a lightweight sentence transformer model\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')  # <100MB, works offline\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path):\n",
        "        \"\"\"Extract all text from a PDF.\"\"\"\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    text += page.extract_text() or \"\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {pdf_path}: {str(e)}\")\n",
        "        return text.strip()\n",
        "\n",
        "    def rank_pdfs(self, pdf_paths, query):\n",
        "        \"\"\"\n",
        "        Rank PDFs by relevance to a query.\n",
        "\n",
        "        Args:\n",
        "            pdf_paths (list): List of PDF file paths.\n",
        "            query (str): The search query (e.g., \"Plan a 4-day trip for college friends\").\n",
        "\n",
        "        Returns:\n",
        "            list: Ranked list of tuples (pdf_path, relevance_score).\n",
        "        \"\"\"\n",
        "        # Encode the query\n",
        "        query_embedding = self.model.encode(query)\n",
        "\n",
        "        # Process each PDF\n",
        "        scores = []\n",
        "        for pdf_path in pdf_paths:\n",
        "            text = self.extract_text_from_pdf(pdf_path)\n",
        "            if not text:\n",
        "                scores.append((pdf_path, 0.0))\n",
        "                continue\n",
        "\n",
        "            # Encode the PDF text\n",
        "            text_embedding = self.model.encode(text)\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarity = cosine_similarity(\n",
        "                [query_embedding],\n",
        "                [text_embedding]\n",
        "            )[0][0]\n",
        "\n",
        "            scores.append((pdf_path, float(similarity)))\n",
        "\n",
        "        # Sort by relevance (highest first)\n",
        "        scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        return scores\n",
        "\n",
        "# Example Usage\n",
        "pdf_paths = [\n",
        "    \"South of France - Cities.pdf\",\n",
        "    \"South of France - Cuisine.pdf\",\n",
        "    \"South of France - History.pdf\",\n",
        "    \"South of France - Restaurants and Hotels.pdf\",\n",
        "    \"South of France - Things to Do.pdf\",\n",
        "    \"South of France - Tips and Tricks.pdf\",\n",
        "    \"South of France - Traditions and Culture.pdf\"\n",
        "]\n",
        "\n",
        "query = \"Plan a 4-day trip for a group of 10 college friends.\"\n",
        "\n",
        "ranker = PDFRanker()\n",
        "ranked_pdfs = ranker.rank_pdfs(pdf_paths, query)\n",
        "\n",
        "# Print results\n",
        "print(\"Ranked PDFs (Most Relevant First):\")\n",
        "for i, (pdf_path, score) in enumerate(ranked_pdfs, 1):\n",
        "    print(f\"{i}. {os.path.basename(pdf_path)} (Score: {score:.3f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up6hfKWasYFO",
        "outputId": "d6866154-7640-401a-fdc3-51d0a5ae8299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked PDFs (Most Relevant First):\n",
            "1. South of France - Tips and Tricks.pdf (Score: 0.246)\n",
            "2. South of France - Things to Do.pdf (Score: 0.164)\n",
            "3. South of France - Restaurants and Hotels.pdf (Score: 0.144)\n",
            "4. South of France - Cities.pdf (Score: 0.101)\n",
            "5. South of France - Cuisine.pdf (Score: 0.065)\n",
            "6. South of France - Traditions and Culture.pdf (Score: 0.048)\n",
            "7. South of France - History.pdf (Score: 0.044)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5kt1m7Attx0",
        "outputId": "6fdbee03-c3c2-45dd-e081-a06093ddf195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lb4tlKqzopY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}